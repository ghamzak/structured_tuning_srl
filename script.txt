install perl
install tcsh
echo 'alias zcat /bin/zcat'  >> ~/.tcshrc	# in case you get "zcat: Command not found"
echo 'alias gzip /bin/gzip'  >> ~/.tcshrc	# in case you get "gzip: Command not found"
echo 'alias rm /bin/rm'  >> ~/.tcshrc	# in case you get "rm: Command not found"

pip install sacremoses	# used by huggingface
#################### CONLL 2005
cd ./data/srl
wget http://www.lsi.upc.edu/~srlconll/conll05st-release.tar.gz
wget http://www.lsi.upc.edu/~srlconll/conll05st-tests.tar.gz
tar xf conll05st-release.tar.gz
tar xf conll05st-tests.tar.gz
# get evaluation script
wget http://www.lsi.upc.es/~srlconll/srl-eval.pl
# get perl dependency
wget https://www.cs.upc.edu/~srlconll/srlconll-1.1.tgz
tar xf srlconll-1.1.tgz

cd conll_extract/
./make_conll2005_data.sh ../data/treebank_3/

python3 preprocess.py --dir ./data/srl/ --batch_size 24 --bert_type roberta-base --train conll05.train.txt --val conll05.devel.txt --test1 conll05.test.wsj.txt --test2 conll05.test.brown.txt --tokenizer_output conll05 --output conll05

python3 preprocess_frameset.py --roleset_dict conll05.roleset_id.dict --label_dict conll05.label.dict \
--train conll05.train.orig_tok_grouped.txt --val conll05.val.orig_tok_grouped.txt \
--test1 conll05.test1.orig_tok_grouped.txt --test2 conll05.test2.orig_tok_grouped.txt \
--output conll05


GPUID=1
DROP=0.5
BERT=base
HIDDEN=768
USE_GOLD=1
LR=0.00003
EPOCH=30
LOSS=crf
PERC=0.03
WARM=0.1
SEED=1
MODEL=./models/bert_${BERT}_${LOSS}_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
python3 -u train.py --gpuid $GPUID --bert_gpuid $GPUID --dir ./data/srl/ --train_data conll05.train.hdf5 --val_data conll05.val.hdf5 \
	--train_res conll05.train.orig_tok_grouped.txt,conll05.train.frame.hdf5,conll05.frame_pool.hdf5 \
	--val_res conll05.val.orig_tok_grouped.txt,conll05.val.frame.hdf5,conll05.frame_pool.hdf5 \
	 --label_dict conll05.label.dict --num_frame 39 \
	--loss $LOSS --optim adamw_fp16 --epochs $EPOCH --warmup_perc $WARM --learning_rate $LR --dropout $DROP --compact_mode whole_word \
	--bert_type roberta-${BERT} --bert_size $HIDDEN --hidden_size $HIDDEN --use_gold_predicate $USE_GOLD \
	--percent $PERC --val_percent 1 --seed $SEED --conll_output $MODEL --save_file $MODEL | tee ${MODEL}.txt
done


################################## with unique_role,overlap_role


GPUID=0
DROP=0.5
BERT=base
HIDDEN=768
USE_GOLD=1
LR=0.00001
EPOCH=5
LOSS=crf,unique_role,frame_role,overlap_role
SEED=1
PERC=0.03
WARM=0.1
for LAMBD in 1,2,1,0.5; do
	LOAD=./models/bert_${BERT}_crf_lr000003_drop05_gold${USE_GOLD}_epoch30_seed${SEED}_perc${PERC//.}
	MODEL=./models/bert2_${BERT}_${LOSS//,}_lambd${LAMBD//.}_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
	python3 -u train.py --gpuid $GPUID --bert_gpuid $GPUID --dir ./data/srl/ --train_data conll05.train.hdf5 --val_data conll05.val.hdf5 \
	--train_res conll05.train.orig_tok_grouped.txt,conll05.train.frame.hdf5,conll05.frame_pool.hdf5 \
	--val_res conll05.val.orig_tok_grouped.txt,conll05.val.frame.hdf5,conll05.frame_pool.hdf5 \
	--label_dict conll05.label.dict --num_frame 39 \
	--optim adamw_fp16 --epochs $EPOCH --warmup_perc $WARM --learning_rate $LR --dropout $DROP --compact_mode whole_word \
	--bert_type roberta-${BERT} --bert_size $HIDDEN --hidden_size $HIDDEN --use_gold_predicate $USE_GOLD \
	--loss $LOSS --lambd $LAMBD \
	--load $LOAD \
	--percent $PERC --val_percent 1 --seed $SEED --conll_output ${MODEL} --save_file $MODEL | tee ${MODEL}.txt
done



GPUID=0
DROP=0.5
BERT=base
HIDDEN=768
USE_GOLD=1
LR=0.00001
EPOCH=5
LOSS=crf,unique_role,frame_role,overlap_role
LAMBD=1,2,0.5,0.5
SEED=1
PERC=0.03
TEST=val
LOAD=./models/bert2_${BERT}_crf_lambd1_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
#LOAD=./models/bert2_${BERT}_crfunique_role_lambd1,2_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
#LOAD=./models/bert2_${BERT}_crfunique_roleframe_role_lambd1,2,05_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
#LOAD=./models/bert2_${BERT}_crfunique_roleframe_roleoverlap_role_lambd1,1,05,01_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
#LOAD=./models/bert2_${BERT}_${LOSS//,}_lambd${LAMBD//.}_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
MODEL=./models/bert2_${BERT}_${LOSS//,}_lambd${LAMBD//.}_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
python3 -u eval.py --gpuid $GPUID --bert_gpuid $GPUID --dir ./data/srl/ --data conll05.${TEST}.hdf5 \
--res conll05.${TEST}.orig_tok_grouped.txt,conll05.${TEST}.frame.hdf5,conll05.frame_pool.hdf5 \
--label_dict conll05.label.dict --num_frame 39 \
--dropout 0 --compact_mode whole_word \
--bert_type roberta-${BERT} --bert_size $HIDDEN --hidden_size $HIDDEN --use_gold_predicate $USE_GOLD \
--loss $LOSS --lambd $LAMBD \
--conll_output ${MODEL} --load_file ${LOAD} | tee ${MODEL}.testlog.txt

perl srl-eval.pl ${MODEL}.gold.txt ${MODEL}.pred.txt


########################################
########################################
######################################## CONLL 2012
########################################
######################################## 

## generating from ontonotes 5.0 data
	get ontonotes 5.0 release of propbank
	cd conll_extract/
	./skeleton2conll.sh -D ../data/ontonotes-release-5.0/data/files/data/ ../data/srl/conll-formatted-ontonotes-5.0/
	./make_conll2012_data.sh ../data/srl/conll-formatted-ontonotes-5.0/
## or get processed files from
	cd ./data/
	git clone https://github.com/yuchenlin/OntoNotes-5.0-NER-BIO.git
	./make_conll2012_data.sh ../data/OntoNotes-5.0-NER-BIO/conll-formatted-ontonotes-5.0/

python3 extract_frameset.py --dir ./data/propbank-frames/frames/ --output ./data/srl/frameset.txt

python3 preprocess.py --dir ./data/srl/ --batch_size 20 --bert_type roberta-base --max_seq_l 410 --max_num_v 45 --train conll2012.train.txt --val conll2012.devel.txt --test1 conll2012.test.txt --test2 "" --tokenizer_output conll2012 --output conll2012

or

python3 preprocess.py --dir ./data/srl/ --batch_size 20 --bert_type bert-base-uncased --max_seq_l 410 --max_num_v 45 --train conll2012.train.txt --val conll2012.devel.txt --test1 conll2012.test.txt --test2 "" --tokenizer_output conll2012_bert --output conll2012_bert

python3 preprocess_frameset.py --train conll2012_bert.train.orig_tok_grouped.txt \
--val conll2012_bert.val.orig_tok_grouped.txt --test1 conll2012_bert.test1.orig_tok_grouped.txt \
--roleset_dict conll2012_bert.roleset_id.dict --label_dict conll2012_bert.label.dict --output conll2012_bert


GPUID=1
DROP=0.5
BERT=base
HIDDEN=768
USE_GOLD=1
LR=0.00003
EPOCH=30
LOSS=crf
PERC=1
VAL_PERC=1
WARM=0.1
SEED=3
MODEL=./models/bert2012_${BERT}_${LOSS//,}_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
python3 -u train.py --gpuid $GPUID --bert_gpuid $GPUID --dir ./data/srl/ --train_data conll2012.train.hdf5 --val_data conll2012.val.hdf5 \
	--train_res conll2012.train.orig_tok_grouped.txt,conll2012.train.frame.hdf5,conll2012.frame_pool.hdf5 \
	--val_res conll2012.val.orig_tok_grouped.txt,conll2012.val.frame.hdf5,conll2012.frame_pool.hdf5 \
	--label_dict conll2012_bert.label.dict \
	--loss $LOSS --optim adamw_fp16 --epochs $EPOCH --warmup_perc $WARM --learning_rate $LR --dropout $DROP --compact_mode whole_word \
	--bert_type roberta-${BERT} --bert_size $HIDDEN --hidden_size $HIDDEN --use_gold_predicate $USE_GOLD \
	--num_label 129 --percent $PERC --val_percent $VAL_PERC \
	--seed $SEED --conll_output $MODEL --save_file $MODEL | tee ${MODEL}.txt



GPUID=0
DROP=0.5
BERT=base
HIDDEN=768
USE_GOLD=1
LR=0.00003
EPOCH=20
LOSS=crf
PERC=1
VAL_PERC=1
WARM=0.1
SEED=3
MODEL=./models/bert_uncased_2012_${BERT}_${LOSS//,}_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
python3 -u train.py --gpuid $GPUID --bert_gpuid $GPUID --dir ./data/srl/ --train_data conll2012_bert.train.hdf5 --val_data conll2012_bert.val.hdf5 \
	--train_res conll2012_bert.train.orig_tok_grouped.txt,conll2012_bert.train.frame.hdf5,conll2012_bert.frame_pool.hdf5 \
	--val_res conll2012_bert.val.orig_tok_grouped.txt,conll2012_bert.val.frame.hdf5,conll2012_bert.frame_pool.hdf5 \
	--label_dict conll2012_bert.label.dict \
	--loss $LOSS --optim adamw_fp16 --epochs $EPOCH --warmup_perc $WARM --learning_rate $LR --dropout $DROP --compact_mode whole_word \
	--bert_type bert-${BERT}-uncased --bert_size $HIDDEN --hidden_size $HIDDEN --use_gold_predicate $USE_GOLD \
	--num_label 129 --percent $PERC --val_percent $VAL_PERC \
	--seed $SEED --conll_output $MODEL --save_file $MODEL | tee ${MODEL}.txt



#################### CONLL 2012 with unique_role,overlap_role

GPUID=0
DROP=0.5
BERT=base
HIDDEN=768
USE_GOLD=1
LR=0.00001
EPOCH=5
PERC=1
VAL_PERC=1
WARM=0.1
LOSS=crf
LAMBD=1
for SEED in 2 3; do
	LOAD=./models/bert_uncased_2012_${BERT}_crf_lr000003_drop${DROP//.}_gold${USE_GOLD}_epoch20_seed${SEED}_perc${PERC//.}
	MODEL=./models/bert2_uncased_2012_${BERT}_${LOSS//,}_lambd${LAMBD//.}_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
	python3 -u train.py --gpuid $GPUID --bert_gpuid $GPUID --dir ./data/srl/ --train_data conll2012_bert.train.hdf5 --val_data conll2012_bert.val.hdf5 \
	--train_res conll2012_bert.train.orig_tok_grouped.txt,conll2012_bert.train.frame.hdf5,conll2012_bert.frame_pool.hdf5 \
	--val_res conll2012_bert.val.orig_tok_grouped.txt,conll2012_bert.val.frame.hdf5,conll2012_bert.frame_pool.hdf5 \
	--label_dict conll2012_bert.label.dict \
	--optim adamw_fp16 --epochs $EPOCH --warmup_perc $WARM --learning_rate $LR --dropout $DROP --compact_mode whole_word \
	--bert_type bert-${BERT}-uncased --bert_size $HIDDEN --hidden_size $HIDDEN --use_gold_predicate $USE_GOLD \
	--num_label 129 --percent $PERC --val_percent $VAL_PERC --loss $LOSS --lambd $LAMBD \
	--load $LOAD \
	--seed $SEED --conll_output ${MODEL} --save_file $MODEL | tee ${MODEL}.txt
done



GPUID=0
DROP=0.5
BERT=base
HIDDEN=768
USE_GOLD=1
LR=0.00001
EPOCH=5
SEED=3
PERC=1
#LOSS=crf,unique_role,frame_role,overlap_role
#LAMBD=1,1,1,0.1
LOSS=crf
LAMBD=1
TEST=test1
MODEL=./models/bert2_2012_${BERT}_${LOSS//,}_lambd${LAMBD//.}_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
python3 -u eval.py --gpuid $GPUID --bert_gpuid $GPUID --dir ./data/srl/ --data conll2012.${TEST}.hdf5 \
--res conll2012.${TEST}.orig_tok_grouped.txt,conll2012.${TEST}.frame.hdf5,conll2012.frame_pool.hdf5 \
--label_dict conll2012.label.dict \
--dropout 0 --compact_mode whole_word \
--bert_type roberta-base --bert_size $HIDDEN --hidden_size $HIDDEN --use_gold_predicate $USE_GOLD \
--num_label 129 --loss $LOSS  --lambd $LAMBD \
--conll_output ${MODEL} --load_file ${MODEL} | tee ${MODEL}.testlog.txt




GPUID=0
DROP=0.5
BERT=base
HIDDEN=768
USE_GOLD=1
LR=0.00001
EPOCH=5
SEED=3
PERC=1
LOSS=crf,unique_role,frame_role,overlap_role
LAMBD=1,0.5,1,0.1
TEST=test1
#MODEL=./models/bert2_uncased_2012_${BERT}_crf_lambd1_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
#MODEL=./models/bert2_uncased_2012_${BERT}_crfunique_role_lambd1,1_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
#MODEL=./models/bert2_uncased_2012_${BERT}_crfunique_roleframe_role_lambd1,05,1_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
MODEL=./models/bert2_uncased_2012_${BERT}_${LOSS//,}_lambd${LAMBD//.}_lr${LR//.}_drop${DROP//.}_gold${USE_GOLD}_epoch${EPOCH}_seed${SEED}_perc${PERC//.}
python3 -u eval.py --gpuid $GPUID --bert_gpuid $GPUID --dir ./data/srl/ --data conll2012_bert.${TEST}.hdf5 \
--res conll2012_bert.${TEST}.orig_tok_grouped.txt,conll2012_bert.${TEST}.frame.hdf5,conll2012_bert.frame_pool.hdf5 \
--label_dict conll2012_bert.label.dict \
--dropout 0 --compact_mode whole_word \
--bert_type bert-${BERT}-uncased --bert_size $HIDDEN --hidden_size $HIDDEN --use_gold_predicate $USE_GOLD \
--num_label 129 --loss $LOSS  --lambd $LAMBD \
--conll_output ${MODEL} --load_file ${MODEL} | tee ${MODEL}.testlog.txt

perl srl-eval.pl ${MODEL}.gold.txt ${MODEL}.pred.txt


